{
 "metadata": {
  "name": "",
  "signature": "sha256:1f562feadc51d3762880ff6a1caf8043792f9339dd7fd3dd5f613359856e4ebd"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.cluster import KMeans\n",
      "from collections import Counter\n",
      "from scipy.spatial.distance import pdist, squareform\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.cluster.hierarchy import linkage, dendrogram\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "## FILTER AND PARSE THE DATA\n",
      "# 1. Apply k-means clustering to the clues\n",
      "docs = pd.read_csv(\"data/documents.csv\", index_col='id')\n",
      "\n",
      "# MAKE ALL LOWER CASE, AND SPLIT SO WE CAN REMOVE STOPWORDS\n",
      "docs['split_clue'] = docs['clue'].apply(lambda x: x.lower().split() )\n",
      "\n",
      "# REMOVE ALL THE STOP WORDS\n",
      "filtered_clue = []\n",
      "for wrds in docs['split_clue']:\n",
      "    filtered_clue.append([w for w in wrds if not w in stopwords.words('english')])\n",
      "    \n",
      "# PLACE THE CLUES WITHOUT STOPWORDS BACK IN OUR DATAFRAME\n",
      "docs['filtered_clue'] = filtered_clue\n",
      "\n",
      "# JOIN THEM BACK INTO A SINGLE STRING SO WE CAN VECOTRIZE THEM \n",
      "docs['filtered_clue'] = docs['filtered_clue'].apply(lambda x: ' '.join(x) )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## K-MEANS ON THE TEXT OF THE QUESTIONS\n",
      "\n",
      "# 1. Apply k-means clustering to the clues\n",
      "vectorizer = TfidfVectorizer(stop_words='english', min_df=10)\n",
      "\n",
      "X = vectorizer.fit_transform(docs['filtered_clue'].values)\n",
      "\n",
      "features = vectorizer.get_feature_names()\n",
      "kmeans = KMeans(n_clusters=10, max_iter=600)\n",
      "kmeans.fit(X)\n",
      "\n",
      "\n",
      "# # 2. Print out the centroids.\n",
      "print \"cluster centers:\"\n",
      "print kmeans.cluster_centers_\n",
      "\n",
      "\n",
      "# # 3. Find the top 10 features for each cluster.\n",
      "top_centroids = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
      "print \"top features for each cluster:\"\n",
      "for num, centroid in enumerate(top_centroids):\n",
      "    print \"%d: %s\" % (num, \", \".join(features[i] for i in centroid))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 5. Print out the titles of a random sample of the articles assigned to each\n",
      "# cluster to get a sense of the topic.\n",
      "assigned_cluster = kmeans.transform(X).argmin(axis=1)\n",
      "for i in range(kmeans.n_clusters):\n",
      "    cluster = np.arange(0, X.shape[0])[assigned_cluster==i]\n",
      "    sample_articles = np.random.choice(cluster, 5, replace=False)\n",
      "    print \"cluster %d:\" % i\n",
      "    for article in sample_articles:\n",
      "        print \"    %s\" % docs.ix[article]['clue']\n",
      "        print \" \\t   %s\" % docs.ix[article]['answer']\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}